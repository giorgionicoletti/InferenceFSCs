{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd35631-105c-47ca-8348-ad9499beb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a739250d-8f22-410a-9e56-326fc845ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import cm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import softmax as softmax\n",
    "from math import ceil\n",
    "from IPython.display import display, clear_output\n",
    "from utils import *\n",
    "sm0 = nn.Softmax(dim = 0)\n",
    "sm1 = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61abd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93df2fcc-facb-40d0-9ab3-32ce5142aaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 2          #number of observations\n",
    "M = 2          #number of memory states\n",
    "A = 2          #number of actions\n",
    "F = 1 + 1      #number of linear features (+ bias)\n",
    "\n",
    "# optimizer parameters\n",
    "lr = 0.01\n",
    "n_epochs = 10\n",
    "n_batch = 50\n",
    "threshold_act = 30\n",
    "dx = 125\n",
    "\n",
    "# trajectories limited?\n",
    "maxT = 5000 \n",
    "\n",
    "# shuffle gradient position\n",
    "x_pos_shuffle = False\n",
    "x_max_shuffle = 5\n",
    "\n",
    "# percentage of training data\n",
    "train_perc = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf05adf-aa3b-497b-b535-bc15bc6c0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the trajectories\n",
    "# name_traj = \"./samples/trjs_N500len500test.pkl\"\n",
    "# with open(name_traj, \"rb\") as f:\n",
    "#     trjs_dict = pickle.load(f)\n",
    "\n",
    "# # Splitting into train and test\n",
    "# Neps = len(trjs_dict)\n",
    "# Ntrain = int(Neps*train_perc/100)\n",
    "\n",
    "# trjs_train = trjs_dict[:Ntrain]\n",
    "# trjs_test = trjs_dict[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412696f-a294-4df9-acbc-c1bb8ac7162c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c107dda1-b68f-441b-8327-9d7213f9598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for softmax policy\n",
    "# dims: {Y, M, M', A} \n",
    "# F = #features + bias\n",
    "\n",
    "#Start from a random theta ---default!\n",
    "theta = torch.rand( (F, M, M, A), requires_grad=True, device='cuda')\n",
    "theta.data = 2*theta.data-1\n",
    "theta.data /= 5\n",
    "\n",
    "\n",
    "#If you restart theta from a saved file\n",
    "theta_is_from_restart = False\n",
    "\n",
    "#If you start theta from a fixed set of numbers\n",
    "theta_is_fixed = False\n",
    "\n",
    "if theta_is_from_restart:\n",
    "    theta_restart_file = \"./results/virtual_data_antonio_FSC/theta_bacteria_FSC_M2_loglike431.11_th30_MselfconsTrue_FromRandom_FSCtrajs.dat\"\n",
    "    theta = torch.from_numpy(np.loadtxt(theta_restart_file).reshape(F,M,M,A))\n",
    "elif theta_is_fixed:\n",
    "    theta=np.array([[[[-0.61114431,-0.55987562],[ 0.59255672,0.82834098]],[[-0.88220033,0.46330714],[ 0.43192955,-0.52351326]]],\n",
    "               [[[ 0.41101729,-0.31575377],[-0.64954147,0.36582982]],[[-0.24503104,0.00449758],[ 0.8396274,0.59647384]]]])\n",
    "    theta=torch.from_numpy(theta.reshape(F,M,M,A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9c681b-c7ea-4cca-a7bf-836458f3299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for softmax of psi to get rho(m_0)\n",
    "psi=torch.rand(M,requires_grad=True, device='cuda')\n",
    "psi_uniform=False\n",
    "if psi_uniform:\n",
    "    psi=torch.ones(M,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac66bff-30d3-4870-9969-dcc67385860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm0 = nn.Softmax(dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e2275-d20f-48c3-aca0-fa80493cd1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "364064b0-b13e-4e9d-9c26-87fe5fd0049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Produce trajectories\n",
    "#ground_truth_theta=np.array([[[[-0.61114431,-0.55987562],[ 0.59255672,0.82834098]],[[-0.88220033,0.46330714],[ 0.43192955,-0.52351326]]], \n",
    "#[[[ 0.41101729,-0.31575377],[-0.64954147,0.36582982]],[[-0.24503104,0.00449758],[ 0.8396274,0.59647384]]]])\n",
    "\n",
    "\n",
    "Ntraj=500\n",
    "traj_len=100\n",
    "signal=get_signal_landscape('step_like',traj_len,Ntraj)\n",
    "dict_trajectories,original_theta = get_traj_from_theta(F,M,A,signal,'False',Ntraj,traj_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99ac1167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.9629398 ,  0.14707666],\n",
       "         [-0.83063267,  0.88302487]],\n",
       "\n",
       "        [[-0.21978671, -0.86869579],\n",
       "         [-0.87569959, -0.42727133]]],\n",
       "\n",
       "\n",
       "       [[[-0.74046244, -0.11874317],\n",
       "         [-0.8246854 ,  0.62118121]],\n",
       "\n",
       "        [[-0.0649276 , -0.23490731],\n",
       "         [-0.14607044, -0.30270079]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27d3c01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0.274, 0.726])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.hist([dict[\"memories\"][0] for dict in dict_trajectories])\n",
    "plt.show()\n",
    "\n",
    "np.unique([dict[\"memories\"][0] for dict in dict_trajectories], return_counts=True)[1]/Ntraj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75886077-7ee0-489b-9745-e75459c5460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trjs_dict=dict_trajectories\n",
    "# Splitting into train and test\n",
    "Neps = len(trjs_dict)\n",
    "Ntrain = int(Neps*train_perc/100)\n",
    "\n",
    "trjs_train = trjs_dict[:Ntrain]\n",
    "trjs_test = trjs_dict[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9abaff09-1211-4146-9c32-9096eee7a7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.1450, -0.0189],\n",
       "           [ 0.1528,  0.1927]],\n",
       " \n",
       "          [[ 0.0591,  0.0914],\n",
       "           [ 0.1113, -0.1871]]],\n",
       " \n",
       " \n",
       "         [[[-0.1760,  0.0465],\n",
       "           [ 0.0093,  0.0971]],\n",
       " \n",
       "          [[-0.1522, -0.1391],\n",
       "           [-0.0617,  0.0541]]]], device='cuda:0', requires_grad=True),\n",
       " tensor([0.3416, 0.4836], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta,psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c1c472c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4646, 0.5354], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=0)(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8655899",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fb7108-af05-412e-a75b-1716ca8c5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgio/miniconda3/envs/minpy/lib/python3.11/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss train: 68.65822996919393\n",
      "\t Initial distribution: tensor([0.5023, 0.4977], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 1 \tLoss train: 66.89736899444246\n",
      "\t Initial distribution: tensor([0.5195, 0.4805], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 2 \tLoss train: 66.38582924331207\n",
      "\t Initial distribution: tensor([0.5003, 0.4997], grad_fn=<SoftmaxBackward0>)\n",
      "Batch 3/8: loss 66.32115620379817\r"
     ]
    }
   ],
   "source": [
    "\n",
    "# use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "theta = theta.to(device)\n",
    "psi = psi.to(device)\n",
    "\n",
    "optimizer_theta = torch.optim.Adam([theta], lr=lr)\n",
    "optimizer_psi = torch.optim.Adam([psi], lr=lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([theta, psi], lr=lr)\n",
    "\n",
    "# if you want to use the same device for all tensors\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "count = 0\n",
    "\n",
    "Neps = len(trjs_dict)\n",
    "\n",
    "lr_mav = 1. / Ntrain\n",
    "\n",
    "losses_train_theta = []\n",
    "losses_test_theta = []\n",
    "\n",
    "losses_train_psi = []\n",
    "losses_test_psi = []\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "alternate_update = False\n",
    "\n",
    "grad_required=True\n",
    "for epochs in range(n_epochs):\n",
    "\n",
    "    running_loss_theta = 0.\n",
    "    running_loss_psi = 0.\n",
    "    running_loss = 0.\n",
    "    random.shuffle(trjs_train)\n",
    "\n",
    "    for ibatch, batch in enumerate(batched(trjs_train, n_batch)):\n",
    "        if alternate_update:\n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "            loss_theta.backward()\n",
    "            running_loss_theta += loss_theta.item()\n",
    "                \n",
    "            optimizer_theta.step()\n",
    "            optimizer_theta.zero_grad()\n",
    "            \n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_psi= trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "            loss_psi.backward()\n",
    "            running_loss_psi += loss_psi.item()\n",
    "                \n",
    "            optimizer_psi.step()\n",
    "            optimizer_psi.zero_grad()\n",
    "            \n",
    "            \n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_psi {loss_psi.item()}\")\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_theta {loss_theta.item()}\", end='\\r')\n",
    "        else:\n",
    "            # update both theta and psi\n",
    "            loss = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss {loss.item()}\", end='\\r')\n",
    "    if alternate_update:\n",
    "        loss_test_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "        loss_test_psi = trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "    \n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_theta/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_psi}\")\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_psi/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_theta}\")\n",
    "        losses_train_theta.append(running_loss_theta)\n",
    "        losses_train_psi.append(running_loss_psi)\n",
    "        losses_test_theta.append(loss_test_theta)\n",
    "        losses_test_psi.append(loss_test_psi)\n",
    "    else:\n",
    "        loss_test = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss/ceil(Ntrain/n_batch)}\")\n",
    "        losses_train.append(running_loss)\n",
    "        losses_test.append(loss_test)\n",
    "\n",
    "        print(f\"\\t Initial distribution: {nn.Softmax(dim=0)(psi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b7436043",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20961e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss train: 55.060594883380574\n",
      "\t Initial distribution: tensor([0.8511, 0.1489], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 1 \tLoss train: 55.05383537603025\n",
      "\t Initial distribution: tensor([0.8428, 0.1572], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 2 \tLoss train: 55.047555039108154\n",
      "\t Initial distribution: tensor([0.8356, 0.1644], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 3 \tLoss train: 55.042400453463365\n",
      "\t Initial distribution: tensor([0.8294, 0.1706], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 4 \tLoss train: 55.03781994519788\n",
      "\t Initial distribution: tensor([0.8256, 0.1744], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "theta = theta.to(device)\n",
    "psi = psi.to(device)\n",
    "\n",
    "optimizer_theta = torch.optim.Adam([theta], lr=lr)\n",
    "optimizer_psi = torch.optim.Adam([psi], lr=lr)\n",
    "\n",
    "optimizer = torch.optim.Adam([theta, psi], lr=lr)\n",
    "\n",
    "# if you want to use the same device for all tensors\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "count = 0\n",
    "\n",
    "Neps = len(trjs_dict)\n",
    "\n",
    "lr_mav = 1. / Ntrain\n",
    "\n",
    "losses_train_theta = []\n",
    "losses_test_theta = []\n",
    "\n",
    "losses_train_psi = []\n",
    "losses_test_psi = []\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "\n",
    "alternate_update = False\n",
    "\n",
    "grad_required=True\n",
    "for epochs in range(n_epochs):\n",
    "\n",
    "    running_loss_theta = 0.\n",
    "    running_loss_psi = 0.\n",
    "    running_loss = 0.\n",
    "    random.shuffle(trjs_train)\n",
    "\n",
    "    for ibatch, batch in enumerate(batched(trjs_train, n_batch)):\n",
    "        if alternate_update:\n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "            loss_theta.backward()\n",
    "            running_loss_theta += loss_theta.item()\n",
    "                \n",
    "            optimizer_theta.step()\n",
    "            optimizer_theta.zero_grad()\n",
    "            \n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_psi= trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "            loss_psi.backward()\n",
    "            running_loss_psi += loss_psi.item()\n",
    "                \n",
    "            optimizer_psi.step()\n",
    "            optimizer_psi.zero_grad()\n",
    "            \n",
    "            \n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_psi {loss_psi.item()}\")\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_theta {loss_theta.item()}\", end='\\r')\n",
    "        else:\n",
    "            # update both theta and psi\n",
    "            loss = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss {loss.item()}\", end='\\r')\n",
    "    if alternate_update:\n",
    "        loss_test_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "        loss_test_psi = trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "    \n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_theta/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_psi}\")\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_psi/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_theta}\")\n",
    "        losses_train_theta.append(running_loss_theta)\n",
    "        losses_train_psi.append(running_loss_psi)\n",
    "        losses_test_theta.append(loss_test_theta)\n",
    "        losses_test_psi.append(loss_test_psi)\n",
    "    else:\n",
    "        loss_test = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss/ceil(Ntrain/n_batch)}\")\n",
    "        losses_train.append(running_loss)\n",
    "        losses_test.append(loss_test)\n",
    "\n",
    "        print(f\"\\t Initial distribution: {nn.Softmax(dim=0)(psi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bef1684a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss train: 55.03349660062381\n",
      "\t Initial distribution: tensor([0.8259, 0.1741], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 1 \tLoss train: 55.02920592334819\n",
      "\t Initial distribution: tensor([0.8301, 0.1699], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 2 \tLoss train: 55.02449706772866\n",
      "\t Initial distribution: tensor([0.8368, 0.1632], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 3 \tLoss train: 55.0192174497293\n",
      "\t Initial distribution: tensor([0.8449, 0.1551], grad_fn=<SoftmaxBackward0>)\n",
      "Epoch: 4 \tLoss train: 55.01332293934659\n",
      "\t Initial distribution: tensor([0.8540, 0.1460], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "grad_required=True\n",
    "for epochs in range(n_epochs):\n",
    "\n",
    "    running_loss_theta = 0.\n",
    "    running_loss_psi = 0.\n",
    "    running_loss = 0.\n",
    "    random.shuffle(trjs_train)\n",
    "\n",
    "    for ibatch, batch in enumerate(batched(trjs_train, n_batch)):\n",
    "        if alternate_update:\n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "            loss_theta.backward()\n",
    "            running_loss_theta += loss_theta.item()\n",
    "                \n",
    "            optimizer_theta.step()\n",
    "            optimizer_theta.zero_grad()\n",
    "            \n",
    "            # Update theta while keeping psi fixed\n",
    "            loss_psi= trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "            loss_psi.backward()\n",
    "            running_loss_psi += loss_psi.item()\n",
    "                \n",
    "            optimizer_psi.step()\n",
    "            optimizer_psi.zero_grad()\n",
    "            \n",
    "            \n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_psi {loss_psi.item()}\")\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss_theta {loss_theta.item()}\", end='\\r')\n",
    "        else:\n",
    "            # update both theta and psi\n",
    "            loss = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Batch {ibatch+1}/{ceil(Ntrain/n_batch)}: loss {loss.item()}\", end='\\r')\n",
    "    if alternate_update:\n",
    "        loss_test_theta = trajs_loss_eval(theta, psi.detach(), batch, trjs_train)\n",
    "        loss_test_psi = trajs_loss_eval(theta.detach(), psi, batch, trjs_train)\n",
    "    \n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_theta/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_psi}\")\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss_psi/ceil(Ntrain/n_batch)} \\tLoss test: {loss_test_theta}\")\n",
    "        losses_train_theta.append(running_loss_theta)\n",
    "        losses_train_psi.append(running_loss_psi)\n",
    "        losses_test_theta.append(loss_test_theta)\n",
    "        losses_test_psi.append(loss_test_psi)\n",
    "    else:\n",
    "        loss_test = trajs_loss_eval(theta, psi, batch, trjs_train)\n",
    "        print(f\"Epoch: {epochs} \\tLoss train: {running_loss/ceil(Ntrain/n_batch)}\")\n",
    "        losses_train.append(running_loss)\n",
    "        losses_test.append(loss_test)\n",
    "\n",
    "        print(f\"\\t Initial distribution: {nn.Softmax(dim=0)(psi)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2195ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8540, 0.1460], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Softmax(dim=0)(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8c58e7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0362, -0.0627],\n",
       "          [-0.2971,  0.4285]],\n",
       "\n",
       "         [[ 0.6432, -0.7863],\n",
       "          [-0.2410,  0.0240]]],\n",
       "\n",
       "\n",
       "        [[[-0.2395,  0.4461],\n",
       "          [-0.0655,  0.0299]],\n",
       "\n",
       "         [[ 0.3534, -0.3930],\n",
       "          [-0.0440,  0.3929]]]], requires_grad=True)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "91eb0281-f34f-4454-9276-d0654cc9a83a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.23064303, 0.22461489, 0.17767354, 0.36706855]),\n",
       " array([0.4564469 , 0.10928453, 0.18852946, 0.24573911]))"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_f(theta.detach().cpu().numpy(), f_traj[1], 0), pi_f(theta.detach().cpu().numpy(), f_traj[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4251afd8-54cc-4ccf-ba7b-950a4e2f1507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14781318, 0.17739546, 0.47524408, 0.19954728]),\n",
       " array([0.18376921, 0.43676727, 0.25392884, 0.12553467]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_f(original_theta, f_traj[0], 0), pi_f(original_theta, f_traj[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "783bb722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.1155057 , 0.08897865],\n",
       "        [0.22785895, 0.09411418]]),\n",
       " array([[0.11248681, 0.1838274 ],\n",
       "        [0.054555  , 0.12267332]]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_Tmm(theta.detach().cpu().numpy(), f_traj[1], 0), np_Tmm(theta.detach().cpu().numpy(), f_traj[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "49e62630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.06244315, 0.20076516],\n",
       "        [0.10613657, 0.14665752]]),\n",
       " array([[0.07494007, 0.08429803],\n",
       "        [0.25225651, 0.072503  ]]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_Tmm(original_theta, f_traj[0], 0), np_Tmm(original_theta, f_traj[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c216bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
